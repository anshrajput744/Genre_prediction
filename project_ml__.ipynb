{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "679a7714-443d-45f2-a1b3-ed21aac20338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googlesearch-pythonNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from googlesearch-python) (4.12.3)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from googlesearch-python) (2.32.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2024.7.4)\n",
      "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: googlesearch-python\n",
      "Successfully installed googlesearch-python-1.3.0\n"
     ]
    }
   ],
   "source": [
    "pip install googlesearch-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "774a2e8f-faea-485d-9a6d-cfc4bd3ba125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f265aa9a-f0ae-433b-a707-589cdfbf9ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from requests) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401d8f09-d775-4d9f-9491-f169cdd4edb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sumy\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting breadability>=0.1.20 (from sumy)\n",
      "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests>=2.7.0 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from sumy) (2.32.2)\n",
      "Collecting pycountry>=18.2.23 (from sumy)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: nltk>=3.0.2 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from sumy) (3.8.1)\n",
      "Requirement already satisfied: chardet in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
      "Requirement already satisfied: lxml>=2.0 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from breadability>=0.1.20->sumy) (5.2.1)\n",
      "Requirement already satisfied: click in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (4.66.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hpsedc\\anaconda3\\lib\\site-packages (from click->nltk>=3.0.2->sumy) (0.4.6)\n",
      "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.3 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 92.2/97.3 kB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 92.2/97.3 kB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 97.3/97.3 kB 617.6 kB/s eta 0:00:00\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/6.3 MB 5.6 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.3/6.3 MB 4.2 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.5/6.3 MB 4.2 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.7/6.3 MB 3.8 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.8/6.3 MB 3.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.0/6.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.2/6.3 MB 3.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.4/6.3 MB 3.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.5/6.3 MB 3.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.7/6.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.9/6.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.0/6.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.2/6.3 MB 3.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.4/6.3 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.5/6.3 MB 3.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.7/6.3 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.9/6.3 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.0/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.2/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.4/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.6/6.3 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.8/6.3 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.9/6.3 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.1/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.4/6.3 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.8/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.9/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.0/6.3 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.2/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.4/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.7/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.0/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.0/6.3 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.1/6.3 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.2/6.3 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 2.8 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: breadability, docopt\n",
      "  Building wheel for breadability (setup.py): started\n",
      "  Building wheel for breadability (setup.py): finished with status 'done'\n",
      "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21739 sha256=2b62045962272c85aad374aac7cab85cc8644ce11a4d1b469a94da706b0e61e5\n",
      "  Stored in directory: c:\\users\\hpsedc\\appdata\\local\\pip\\cache\\wheels\\32\\99\\64\\59305409cacd03aa03e7bddf31a9db34b1fa7033bd41972662\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13773 sha256=5954ac218c98e96c7c4a35aa949d71bb7e2defd827c8e145cc4d47c7e8de9f89\n",
      "  Stored in directory: c:\\users\\hpsedc\\appdata\\local\\pip\\cache\\wheels\\1a\\bf\\a1\\4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
      "Successfully built breadability docopt\n",
      "Installing collected packages: docopt, pycountry, breadability, sumy\n",
      "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9096dc84-ce74-4a54-ba8b-9050f41d0fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, messagebox\n",
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "\n",
    "# -------------------- Load and Prepare Dataset --------------------\n",
    "df = pd.read_csv(\"goodreads_data.csv\")\n",
    "df_rec['Book'] = df_rec['Book'].str.lower()\n",
    "title_to_index = {title: idx for idx, title in enumerate(df_rec['Book'])}\n",
    "df_cleaned = df.dropna(subset=['Book', 'Genres'])\n",
    "df_cleaned['Genres'] = df_cleaned['Genres'].apply(lambda x: \" \".join(eval(x)) if isinstance(x, str) else \"\")\n",
    "\n",
    "# For genre recommendation\n",
    "df_rec = df_cleaned.copy()\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(df_rec['Genres'])\n",
    "\n",
    "nn_model = NearestNeighbors(n_neighbors=6, algorithm='auto', metric='cosine')\n",
    "nn_model.fit(tfidf_matrix)\n",
    "\n",
    "# For genre prediction\n",
    "df_pred = df_cleaned[df_cleaned['Genres'].str.strip().astype(bool)].copy()\n",
    "tfidf_features = vectorizer.fit_transform(df_pred['Book'])\n",
    "\n",
    "genre_labels = df_pred['Genres']\n",
    "genre_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "genre_clf.fit(tfidf_features, genre_labels)\n",
    "\n",
    "title_to_index = pd.Series(df_rec.index, index=df_rec['Book'].str.lower())\n",
    "\n",
    "# -------------------- Functions --------------------\n",
    "def get_google_links(book_name, sites):\n",
    "    links = {}\n",
    "    for site in sites:\n",
    "        query = f\"{book_name} book site:{site}\"\n",
    "        for result in search(query, num_results=5):\n",
    "            if site in result:\n",
    "                links[site] = result\n",
    "                break\n",
    "    return links\n",
    "\n",
    "def get_wikipedia_summary(wiki_url):\n",
    "    try:\n",
    "        response = requests.get(wiki_url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        paragraphs = soup.select('p')\n",
    "        summary = \"\"\n",
    "        for para in paragraphs:\n",
    "            text = para.get_text().strip()\n",
    "            if text and not text.lower().startswith(\"coordinates\"):\n",
    "                summary += text + \"\\n\\n\"\n",
    "            if len(summary.split()) > 120:\n",
    "                break\n",
    "        return summary.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching summary: {str(e)}\"\n",
    "\n",
    "def get_recommendations_by_genre_input(genre, top_n=5):\n",
    "    matching_books = df_rec[df_rec['Genres'].str.lower().str.contains(str(genre).lower(), na=False)]\n",
    "    if matching_books.empty:\n",
    "        return [\"No recommendations found.\"]\n",
    "    idx = matching_books.index[0]\n",
    "    distances, indices = nn_model.kneighbors(tfidf_matrix[idx], n_neighbors=top_n + 1)\n",
    "    return df_rec.iloc[indices[0][1:]]['Book'].tolist()\n",
    "\n",
    "def predict_genre(book_title):\n",
    "    book_vector = vectorizer.transform([book_title])\n",
    "    predicted_genre = genre_clf.predict(book_vector)[0]\n",
    "    return predicted_genre\n",
    "\n",
    "# -------------------- Main Book Search --------------------\n",
    "def search_book():\n",
    "    book_name = entry.get().strip()\n",
    "    if not book_name:\n",
    "        messagebox.showwarning(\"Input Error\", \"Please enter a book name.\")\n",
    "        return\n",
    "\n",
    "    summary_text.delete(1.0, tk.END)\n",
    "\n",
    "    # Wikipedia summary\n",
    "    wiki_link = None\n",
    "    for link in search(f\"{book_name} book site:en.wikipedia.org\", num_results=5):\n",
    "        if \"wikipedia.org\" in link:\n",
    "            wiki_link = link\n",
    "            break\n",
    "\n",
    "    if wiki_link:\n",
    "        summary = get_wikipedia_summary(wiki_link)\n",
    "        summary_text.insert(tk.END, f\"üìù Summary (Wikipedia):\\n{summary}\\n\\n\")\n",
    "        summary_text.insert(tk.END, f\"üîó Wikipedia: {wiki_link}\\n\\n\")\n",
    "    else:\n",
    "        summary_text.insert(tk.END, \"Wikipedia summary not found.\\n\\n\")\n",
    "\n",
    "    # Purchase links\n",
    "    sites = [\"amazon.com\", \"goodreads.com\"]\n",
    "    links = get_google_links(book_name, sites)\n",
    "    for site, link in links.items():\n",
    "        summary_text.insert(tk.END, f\"üîó {site.capitalize()}: {link}\\n\")\n",
    "\n",
    "    # Genre & Recommendations\n",
    "    book_lower = book_name.lower()\n",
    "    if book_lower in title_to_index:\n",
    "        genre_text = df_rec.loc[title_to_index[book_lower], 'Genres']\n",
    "        summary_text.insert(tk.END, f\"\\nüéØ Genre (from dataset): {genre_text}\\n\")\n",
    "        recommendations = get_recommendations_by_genre_input(genre_text)\n",
    "    else:\n",
    "        predicted_genre = predict_genre(book_name)\n",
    "        summary_text.insert(tk.END, f\"\\nü§ñ Predicted Genre: {predicted_genre}\\n\")\n",
    "        recommendations = get_recommendations_by_genre_input(predicted_genre)\n",
    "\n",
    "    summary_text.insert(tk.END, \"\\nüìö Recommended Books (by Genre):\\n\")\n",
    "    for rec in recommendations:\n",
    "        summary_text.insert(tk.END, f\"- {rec}\\n\")\n",
    "\n",
    "# -------------------- UI --------------------\n",
    "root = tk.Tk()\n",
    "root.title(\"üìö Book Finder + Genre Recommendation\")\n",
    "root.geometry(\"750x650\")\n",
    "\n",
    "label = tk.Label(root, text=\"Enter Book Name:\", font=(\"Arial\", 14))\n",
    "label.pack(pady=10)\n",
    "\n",
    "entry = tk.Entry(root, font=(\"Arial\", 14), width=50)\n",
    "entry.pack()\n",
    "\n",
    "button = tk.Button(root, text=\"Search Book\", font=(\"Arial\", 12), command=search_book)\n",
    "button.pack(pady=10)\n",
    "\n",
    "summary_text = scrolledtext.ScrolledText(root, wrap=tk.WORD, font=(\"Arial\", 12))\n",
    "summary_text.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad1cc3-0390-4784-a984-e6e695a5b83f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
